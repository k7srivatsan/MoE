# Patch-wise Mixture of Experts (MoE) on FashionMNIST

A minimal Mixture of Experts (MoE) model built from scratch in PyTorch, using patch-wise routing over FashionMNIST. This project explores sparse expert activation, routing specialization, and tradeoffs in compute, accuracy, and interpretability.

---

## ðŸ”§ Project Highlights

- âœ… Implements a custom **top-k MoE** architecture without HuggingFace
- âœ… Uses **5Ã—5 or 6Ã—6 non-overlapping image patches** as routing tokens
- âœ… Trains with **soft routing**, evaluates with **top-k expert selection**
- âœ… Visualizes per-expert token prototypes and specialization
- âœ… Tracks CE loss, load balancing loss, routing entropy
- âœ… Logs runtime, memory, and routing behavior over epochs

---

## ðŸ§  Model Overview

- **Input**: FashionMNIST image â†’ patchify into 25â€“36 tokens (5Ã—5 patches)
- **Patch encoder**: 2-layer MLP (`25 â†’ 64 â†’ D`)
- **Router**: Softmax-based token-wise gating (`D â†’ num_experts`)
- **Experts**: MLPs (`D â†’ D â†’ D`) with ReLU + Dropout
- **Classifier**: MLP over pooled expert outputs

```text
            +-----------------------------+
Image --->  | Patchify â†’ Patch Encoder    |  â†’ Tokens (T, D)
            +-----------------------------+
                             â†“
                   +-----------------+
                   |   MoE Router    |  â†’ Softmax over E
                   +-----------------+
                            â†“
        +--------------------+--------------------+
        | Expert 0 (MLP)     | ...   Expert N (MLP)|
        +--------------------+--------------------+
                            â†“
             Token Outputs (sparse or weighted)
                            â†“
              Mean Pool â†’ Classifier MLP â†’ Prediction
